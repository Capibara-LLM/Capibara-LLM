{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04af073c",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Capibara-LLM/Capibara-LLM/blob/main/gemma_2_9b_it_SimPO_Jopara.ipynb\"\n",
    "    target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WzU9ChvRshLo",
   "metadata": {
    "id": "WzU9ChvRshLo"
   },
   "source": [
    "Basado en https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809e44e",
   "metadata": {
    "id": "d809e44e"
   },
   "source": [
    "### Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03598f75",
   "metadata": {
    "id": "03598f75"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "     import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "     xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "     !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "     !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d1188",
   "metadata": {
    "id": "925d1188"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73602c",
   "metadata": {
    "id": "ea73602c"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None  # None for auto detection\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"princeton-nlp/gemma-2-9b-it-SimPO\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cd687",
   "metadata": {
    "id": "087cd687"
   },
   "source": [
    "¡Ahora agregamos adaptadores LoRA para que solo necesitemos actualizar entre el 1 y el 10 % de todos los parámetros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafae6d",
   "metadata": {
    "id": "3dafae6d"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,              # Rango de la matriz (16 está bien para eficiencia/calidad)\n",
    "    target_modules = [   # Apuntamos a todos los módulos lineales (Correcto para Gemma 2)\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = 32,     # CAMBIO: Aumentado de 16 a 32 (Regla: alpha = 2 * r)\n",
    "    lora_dropout = 0,    # 0 es correcto para entrenamientos optimizados\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d0779",
   "metadata": {
    "id": "a25d0779"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "Carga de datasets: Rubuntu (Filtrado) + Capibara (Concatenado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a544e",
   "metadata": {
    "id": "f56a544e"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# alpaca_prompt = \"\"\"Iguýpe oĩ peteĩ instrucción omombeꞌuva peteĩ tembiapo, oñembojoajúva peteĩ entrada ndive omeꞌevéva contexto. Ehai peteĩ ñembohovái omohu’ãva hekopete pe mba’ejerure.\n",
    "\n",
    "# ### Instrucción:\n",
    "# {}\n",
    "\n",
    "# ### Entrada:\n",
    "# {}\n",
    "\n",
    "# ### Mbohovái:\n",
    "# {}\"\"\"\n",
    "\n",
    "# EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "#def formatting_prompts_func(examples):\n",
    "#    instructions = examples[\"instruction\"]\n",
    "#    inputs       = examples[\"input\"]\n",
    "#    outputs      = examples[\"output\"]\n",
    "#    texts = []\n",
    "#    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "#        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "#        texts.append(text)\n",
    "#    return { \"text\" : texts, }\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "\n",
    "    # Plantilla completa (cuando hay contexto/input)\n",
    "    prompt_with_input = \"\"\"Iguýpe oĩ peteĩ instrucción omombeꞌuva peteĩ tembiapo, oñembojoajúva peteĩ entrada ndive omeꞌevéva contexto. Ehai peteĩ ñembohovái omohu’ãva hekopete pe mba’ejerure.\n",
    "\n",
    "### Instrucción:\n",
    "{}\n",
    "\n",
    "### Entrada:\n",
    "{}\n",
    "\n",
    "### Mbohovái:\n",
    "{}\"\"\"\n",
    "\n",
    "    # Plantilla simplificada (cuando NO hay input)\n",
    "    # Nota: Quitamos la intro larga en guaraní para instrucciones simples\n",
    "    prompt_no_input = \"\"\"### Instrucción:\n",
    "{}\n",
    "\n",
    "### Mbohovái:\n",
    "{}\"\"\"\n",
    "\n",
    "    EOS_TOKEN = tokenizer.eos_token # Aseguramos que esté disponible\n",
    "\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Si input es None o una cadena vacía/espacios\n",
    "        if input is None or str(input).strip() == \"\":\n",
    "            text = prompt_no_input.format(instruction, output) + EOS_TOKEN\n",
    "        else:\n",
    "            text = prompt_with_input.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# --- CARGA Y PROCESAMIENTO DE DATASETS ---\n",
    "\n",
    "# 1. Cargar Dataset Principal: Rubuntu Guarani-Jopara\n",
    "print(\"Cargando dataset Rubuntu...\")\n",
    "dataset_main = load_dataset(\"rubuntu/dataset-guarani-jopara-v01\", split=\"train\")\n",
    "\n",
    "# FILTRO: Mantener solo filas donde output NO es None y NO está vacío\n",
    "dataset_main = dataset_main.filter(lambda x: x['output'] is not None and len(str(x['output']).strip()) > 0)\n",
    "print(f\"Dataset Rubuntu filtrado: {len(dataset_main)} filas.\")\n",
    "\n",
    "# 2. Cargar Dataset Secundario: Capibara LLM\n",
    "print(\"Cargando dataset Capibara...\")\n",
    "dataset_secondary = load_dataset(\"Capibara-LLM/gn-multi-affective-alpaca\", split=\"train\")\n",
    "print(f\"Dataset Capibara cargado: {len(dataset_secondary)} filas.\")\n",
    "\n",
    "# 3. Estandarización de columnas\n",
    "def standardize_columns(ds):\n",
    "    if 'input' not in ds.column_names:\n",
    "        ds = ds.add_column(\"input\", [\"\"] * len(ds))\n",
    "    return ds.select_columns([\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "dataset_main = standardize_columns(dataset_main)\n",
    "dataset_secondary = standardize_columns(dataset_secondary)\n",
    "\n",
    "# 4. Concatenar y mezclar\n",
    "dataset = concatenate_datasets([dataset_main, dataset_secondary])\n",
    "dataset = dataset.shuffle(seed=3407)\n",
    "print(f\"Total combinado para entrenamiento: {len(dataset)} filas.\")\n",
    "\n",
    "# Aplicar formato\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fab00b",
   "metadata": {
    "id": "73fab00b"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c093e82",
   "metadata": {
    "id": "1c093e82"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2, # Usa 2 núcleos para procesar datos más rápido\n",
    "    packing = True,       # ¡ACTIVADO! Mucho más rápido y eficiente\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8, # Batch efectivo = 16 (más estable)\n",
    "        warmup_steps = 10,               # Un poco más de calentamiento para el entrenamiento completo\n",
    "        # max_steps = 60,                # Entrenar todo el dataset\n",
    "        num_train_epochs = 1,            # 1 pasada completa a los datos (ajusta a 3 si tienes pocos datos)\n",
    "        learning_rate = 2e-4,            # 2e-4 está bien con LoRA alpha=32\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f4122",
   "metadata": {
    "id": "8c9f4122"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9469a183",
   "metadata": {
    "id": "9469a183"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606e018",
   "metadata": {
    "id": "3606e018"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Mba'éichapa reiko?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6d4e7",
   "metadata": {
    "id": "33d6d4e7"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Guardar y cargar modelos ajustados\n",
    "Para guardar el modelo final como adaptadores LoRA, utilizar `push_to_hub` de Huggingface para guardar en línea o `save_pretrained` para guardar localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6952ddf",
   "metadata": {
    "id": "f6952ddf"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "# Define el nombre del modelo\n",
    "model_name_local = \"gemma-2-9b-it-SimPO-Jopara\"\n",
    "repo_id_hub = \"Capibara-LLM/gemma-2-9b-it-SimPO-Jopara\"\n",
    "\n",
    "# Obtener el token desde los secretos de Colab\n",
    "try:\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    if hf_token is None:\n",
    "        print(\"Advertencia: El secreto 'HF_TOKEN' no fue encontrado. Usando valor por defecto.\")\n",
    "        hf_token = \"TU_TOKEN_HUGGINGFACE_AQUI\"\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo cargar el token de secretos: {e}\")\n",
    "    hf_token = \"TU_TOKEN_HUGGINGFACE_AQUI\"\n",
    "\n",
    "print(f\"Guardando localmente en: {model_name_local}\")\n",
    "model.save_pretrained(model_name_local)\n",
    "tokenizer.save_pretrained(model_name_local)\n",
    "\n",
    "print(f\"Subiendo a Hugging Face Hub: {repo_id_hub}\")\n",
    "# Subir automáticamente si tienes el token configurado\n",
    "if hf_token != \"TU_TOKEN_HUGGINGFACE_AQUI\":\n",
    "    model.push_to_hub(repo_id_hub, token = hf_token)\n",
    "    tokenizer.push_to_hub(repo_id_hub, token = hf_token)\n",
    "else:\n",
    "    print(\"Token no configurado correctamente, saltando subida al Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5045a",
   "metadata": {
    "id": "85c5045a"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "Opcional: Si se quiere guardar o subir versiones GGUF, cambiar False a True abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4180d82",
   "metadata": {
    "id": "a4180d82"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # RECOMENDADO: Usar q5_k_m para reducir alucinaciones manteniendo un tamaño razonable\n",
    "    quant_method = \"q5_k_m\"\n",
    "\n",
    "    print(f\"Guardando y subiendo GGUF con método: {quant_method}\")\n",
    "\n",
    "    # Guardar GGUF localmente\n",
    "    model.save_pretrained_gguf(model_name_local, tokenizer, quantization_method=quant_method)\n",
    "\n",
    "    # Subir GGUF al repositorio\n",
    "    model.push_to_hub_gguf(repo_id_hub + \"-GGUF\", tokenizer, quantization_method=quant_method, token=hf_token)\n",
    "\n",
    "if True:\n",
    "    # Subir múltiples formatos GGUF (q4_k_m, q8_0, q5_k_m)\n",
    "    model.push_to_hub_gguf(\n",
    "        repo_id_hub + \"-GGUF\",\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = hf_token,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
